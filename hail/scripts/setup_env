#!/bin/bash

if [[ -z "${SPARK_HOME}" ]]
then
    echo 'Please set the SPARK_HOME variable to the path to a Spark installation, e.g. /opt/spark-2.0.2-bin-hadoop2.7' >&2
    exit -1
fi

spark_core_jars=( "${SPARK_HOME}/jars/spark-core*.jar" )
if [ ${#spark_core_jars[@]} -eq 0 ]
then
    echo "Could not find a spark-core jar in ${SPARK_HOME}/jars, are you sure SPARK_HOME is set correctly?" >&2
    exit -1
fi

export HAIL_HOME="$(dirname "$(cd "$(dirname $0)" && pwd)")"
export PYTHONPATH="$HAIL_HOME/python:$SPARK_HOME/python:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip | tr '\n' ':')$PYTHONPATH"
JAR_PATH="$HAIL_HOME/jars/hail-all-spark.jar"
export PYSPARK_SUBMIT_ARGS="--conf spark.driver.extraClassPath='$JAR_PATH' --conf spark.executor.extraClassPath='$JAR_PATH' pyspark-shell"
